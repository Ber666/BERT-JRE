{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a8910d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:22:59.712868Z",
     "start_time": "2021-07-10T02:22:52.985269Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, IterableDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import *\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from allennlp.nn.util import batched_index_select\n",
    "# from allennlp.modules import FeedForward\n",
    "# import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842544f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:23:19.476435Z",
     "start_time": "2021-07-10T02:23:19.471435Z"
    }
   },
   "outputs": [],
   "source": [
    "# from allennlp.modules import FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209ac304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:23:19.978049Z",
     "start_time": "2021-07-10T02:23:19.961050Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beb394e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:23:20.434955Z",
     "start_time": "2021-07-10T02:23:20.425956Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "599ed411",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:23:20.952719Z",
     "start_time": "2021-07-10T02:23:20.942720Z"
    }
   },
   "outputs": [],
   "source": [
    "ner_list = [\"N/A\", 'FAC', 'WEA', 'LOC', 'VEH', 'GPE', 'ORG', 'PER']\n",
    "re_list = [\"N/A\", 'ART', 'ORG-AFF', 'GEN-AFF', 'PHYS', 'PER-SOC', 'PART-WHOLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a98f8d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T07:14:48.639075Z",
     "start_time": "2021-07-10T07:14:48.625075Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class JREDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_span_len, ner_list, re_list):\n",
    "        super().__init__()\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.raw_data = [sent for sent in json.load(f) if len(sent['tokens']) > 0]\n",
    "        self.ner_id2label = ner_list\n",
    "        self.ner_label2id = {j:i for i, j in enumerate(ner_list)}\n",
    "        self.re_id2label = re_list\n",
    "        self.re_label2id = {j:i for i, j in enumerate(re_list)}\n",
    "        self.data = []\n",
    "        c_ner, c_re, c_span_len = Counter(), Counter(), Counter()\n",
    "        self.c_ori_ner = 0\n",
    "        for l in tqdm(self.raw_data):\n",
    "            sub_token_mapping = []  # (index, len)\n",
    "            refined_tokens = []\n",
    "            cnt = 0\n",
    "            # first re-tokenize the tokens with BertTokenizer\n",
    "            self.c_ori_ner += len(l['entities'])\n",
    "            for t in l['tokens']:\n",
    "                subtokens = tokenizer.tokenize(t)\n",
    "                tmp_len = len(subtokens)\n",
    "                refined_tokens += subtokens\n",
    "                sub_token_mapping.append((cnt, tmp_len))\n",
    "                cnt += tmp_len\n",
    "            \n",
    "            refined_entities = {(sub_token_mapping[e[0]][0], sub_token_mapping[e[1]-1][0] + sub_token_mapping[e[1]-1][1]): e[2] for e in l['entities']}\n",
    "            refined_relations = {(sub_token_mapping[r[0]][0], sub_token_mapping[r[1]-1][0] + sub_token_mapping[r[1]-1][1], \\\n",
    "                sub_token_mapping[r[2]][0], sub_token_mapping[r[3]-1][0] + sub_token_mapping[r[3]-1][1]): r[4] for r in l['relations']}\n",
    "            c_span_len += Counter([j - i for i, j in refined_entities])\n",
    "            spans, spans_label = [], []\n",
    "            # span2id = {}\n",
    "            cnt = 0\n",
    "            for i in range(len(refined_tokens)):\n",
    "                for j in range(i + 1, min(len(refined_tokens), i + max_span_len + 1)):\n",
    "                    spans.append((i, j))\n",
    "                    # span2id[(i, j)] = cnt\n",
    "                    cnt += 1\n",
    "                    spans_label.append(self.ner_label2id[refined_entities.get((i, j), 'N/A')])\n",
    "            entity_pairs, entity_pairs_label = [], []\n",
    "            for s, s_ in refined_entities.items():\n",
    "                for t, t_ in refined_entities.items():\n",
    "                    # if j >= i:\n",
    "                    #     break\n",
    "                    # print(s, s_, t, t_)\n",
    "                    entity_pairs.append((s[0], s[1], t[0], t[1], self.ner_label2id[s_], self.ner_label2id[t_]))\n",
    "                    entity_pairs_label.append(self.re_label2id[refined_relations.get((s[0], s[1], t[0], t[1]), 'N/A')])\n",
    "            refined_tokens_ids = tokenizer.convert_tokens_to_ids(refined_tokens)\n",
    "            self.data.append({\n",
    "                'tokens': refined_tokens,\n",
    "                'tokens_id': [tokenizer.cls_token_id, *refined_tokens_ids, tokenizer.sep_token_id],\n",
    "                'entities': refined_entities,\n",
    "                'relations': refined_relations,\n",
    "                'spans': spans,\n",
    "                'spans_label': spans_label,\n",
    "                'entity_pairs': entity_pairs,\n",
    "                'entity_pairs_label': entity_pairs_label\n",
    "            })\n",
    "            c_ner += Counter(list(refined_entities.values()))\n",
    "            c_re += Counter(list(refined_relations.values()))\n",
    "        n_ner = Counter([len(i['entities']) for i in self.data])\n",
    "        n_re = Counter([len(i['relations']) for i in self.data])\n",
    "        print(\"entity label stats:\", dict(c_ner))\n",
    "        print(\"relation label stats:\", dict(c_re))\n",
    "        print(\"span len stats:\", dict(c_span_len))\n",
    "        print(\"num of entity stats:\", dict(n_ner))\n",
    "        print(\"num of relation stats:\", dict(n_re))\n",
    "            # need stats\n",
    "            # for i in \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e821e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:23:29.771878Z",
     "start_time": "2021-07-10T02:23:22.816408Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "983478a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T07:15:01.388309Z",
     "start_time": "2021-07-10T07:14:59.907303Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2050/2050 [00:01<00:00, 1404.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity label stats: {'GPE': 1020, 'ORG': 837, 'PER': 2967, 'FAC': 291, 'LOC': 136, 'WEA': 109, 'VEH': 116}\n",
      "relation label stats: {'PART-WHOLE': 182, 'PHYS': 278, 'ORG-AFF': 359, 'ART': 151, 'PER-SOC': 77, 'GEN-AFF': 104}\n",
      "span len stats: {1: 4434, 2: 547, 3: 302, 4: 118, 5: 47, 6: 17, 8: 3, 13: 2, 7: 4, 11: 2}\n",
      "num of entity stats: {0: 533, 3: 233, 5: 137, 4: 167, 2: 280, 6: 97, 7: 79, 8: 43, 11: 18, 1: 389, 9: 33, 10: 16, 13: 5, 12: 9, 15: 2, 14: 4, 16: 3, 17: 2}\n",
      "num of relation stats: {0: 1453, 1: 295, 3: 73, 2: 167, 5: 15, 4: 35, 6: 4, 7: 4, 9: 1, 11: 1, 8: 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testset = JREDataset('data/test.ACE05.json', tokenizer, 8, ner_list, re_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eb4ecb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T07:15:04.470064Z",
     "start_time": "2021-07-10T07:15:02.794506Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2424/2424 [00:01<00:00, 1466.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity label stats: {'GPE': 1265, 'ORG': 989, 'PER': 3431, 'FAC': 249, 'LOC': 156, 'VEH': 125, 'WEA': 123}\n",
      "relation label stats: {'ORG-AFF': 365, 'PART-WHOLE': 162, 'PHYS': 278, 'ART': 96, 'PER-SOC': 106, 'GEN-AFF': 124}\n",
      "span len stats: {1: 5151, 2: 629, 4: 118, 3: 357, 5: 44, 8: 4, 6: 21, 7: 6, 12: 3, 10: 1, 15: 1, 11: 2, 9: 1}\n",
      "num of entity stats: {0: 635, 2: 339, 13: 5, 9: 32, 8: 51, 5: 166, 3: 287, 7: 82, 4: 199, 6: 110, 1: 455, 11: 22, 10: 27, 12: 4, 17: 1, 15: 3, 22: 1, 14: 3, 18: 1, 19: 1}\n",
      "num of relation stats: {0: 1793, 6: 10, 1: 357, 4: 26, 3: 72, 2: 149, 7: 2, 5: 13, 8: 1, 9: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validset = JREDataset(\"data/valid.ACE05.json\", tokenizer, 8, ner_list, re_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c69c378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T07:15:12.577882Z",
     "start_time": "2021-07-10T07:15:05.075794Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10051/10051 [00:06<00:00, 1469.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity label stats: {'GPE': 5169, 'VEH': 678, 'PER': 14415, 'ORG': 3781, 'LOC': 827, 'WEA': 679, 'FAC': 921}\n",
      "relation label stats: {'PART-WHOLE': 775, 'ART': 491, 'ORG-AFF': 1472, 'GEN-AFF': 511, 'PHYS': 1097, 'PER-SOC': 438}\n",
      "span len stats: {1: 21184, 2: 2758, 3: 1568, 6: 95, 8: 24, 5: 204, 4: 548, 7: 46, 10: 10, 9: 10, 14: 2, 13: 4, 11: 5, 12: 8, 18: 1, 15: 2, 19: 1}\n",
      "num of entity stats: {0: 2574, 12: 41, 11: 76, 10: 105, 9: 164, 3: 1129, 5: 653, 4: 808, 6: 476, 7: 371, 8: 218, 2: 1320, 1: 2060, 13: 21, 15: 6, 16: 7, 17: 5, 14: 12, 28: 1, 23: 1, 18: 2, 20: 1}\n",
      "num of relation stats: {0: 7408, 4: 141, 3: 294, 1: 1443, 7: 10, 2: 666, 5: 60, 8: 2, 6: 23, 9: 1, 10: 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainset = JREDataset(\"data/train.ACE05.json\", tokenizer, 8, ner_list, re_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1caba66d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:23:56.601473Z",
     "start_time": "2021-07-10T02:23:56.590475Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_collate_fn(batch):\n",
    "    seq = pad_sequence([torch.tensor(i['tokens_id']) for i in batch], padding_value=0, batch_first=True).to('cpu')\n",
    "    mask = (seq > 0).float()\n",
    "    span_index = torch.zeros(len(batch), 2)\n",
    "    cur_len = 0\n",
    "    span_batch = []\n",
    "    span_label_batch = []\n",
    "    for i, j in enumerate(batch):\n",
    "        span_batch += j['spans']\n",
    "        span_index[i][0] = cur_len\n",
    "        tmp_len = len(j['spans'])\n",
    "        span_index[i][1] = tmp_len\n",
    "        cur_len += tmp_len\n",
    "        span_label_batch += j['spans_label']\n",
    "    span_batch = torch.tensor(span_batch).to('cpu').long()\n",
    "    span_index = span_index.to('cpu').long()\n",
    "    span_label_batch = torch.tensor(span_label_batch).to('cpu')\n",
    "    # spans = torch.tensor(inputs['spans'])\n",
    "    return seq, mask, span_batch, span_label_batch, span_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4564b88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:24:06.743060Z",
     "start_time": "2021-07-10T02:24:06.726542Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testset, batch_size=16, shuffle=True, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=my_collate_fn, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1a4571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:24:07.130056Z",
     "start_time": "2021-07-10T02:24:07.123056Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(testset, batch_size=16, shuffle=True, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=my_collate_fn, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c32285bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:24:07.502100Z",
     "start_time": "2021-07-10T02:24:07.485073Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=my_collate_fn, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c69adb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:24:11.907571Z",
     "start_time": "2021-07-10T02:24:09.319540Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6702673e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:24:11.938585Z",
     "start_time": "2021-07-10T02:24:11.925574Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DistilBertNER(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.to('cpu')\n",
    "        self.ner_classifier = nn.Sequential(\n",
    "            nn.Linear(768 * 2 + 0, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(768, 8)\n",
    "        )\n",
    "    def forward(self, seq, mask, span_batch, span_index):\n",
    "        # print(seq.shape)\n",
    "        embs = self.encoder(seq, attention_mask=mask)[0][:, 1:, :]\n",
    "        emb_ls = []\n",
    "        for i, (j, k) in enumerate(span_index):\n",
    "            # print(i, j, k)\n",
    "            # j, k : the start index and length for spans of this batch\n",
    "            spans = span_batch[j: j+k]  # (n_span, 2)\n",
    "            x = embs[i, spans[:, 0], :]  # (n_span, 768)\n",
    "            y = embs[i, spans[:, 1], :]\n",
    "            # y = embs[i, span_batch[j+k][0]: span_batch[j+k][0] + span_batch[j+k][1], :]\n",
    "            emb_ls.append(torch.cat((x, y), dim=-1))\n",
    "        emb_ls = torch.cat(emb_ls, dim=0)\n",
    "        return self.ner_classifier(emb_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2871ca1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:24:19.838142Z",
     "start_time": "2021-07-10T02:24:19.815628Z"
    }
   },
   "outputs": [],
   "source": [
    "model = DistilBertNER(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57180874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T02:24:20.277692Z",
     "start_time": "2021-07-10T02:24:20.266693Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f2e9223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T15:52:42.039767Z",
     "start_time": "2021-07-07T15:52:42.032766Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2017c290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T08:53:33.512523Z",
     "start_time": "2021-07-10T08:51:05.592398Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|                                                                                          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████████████████████████████▍                             | 82/129 [01:33<00:40,  1.05s/it]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 10013 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'cor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-08d345271e03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_valid\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         '''\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cur results on dev:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_ori_ner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-3efb1438a993>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, loader, n_total_ner)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all accuracy: %4f'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'for valid spans: cor: %d, pred: %d, tot: %d, cand tot: %d'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_cor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_total_ner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_tot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcor\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtot_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcor\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtot_gold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cor' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for step, i in tqdm(enumerate(train_loader)):\n",
    "        # break\n",
    "        seq, mask, span_batch, span_label_batch, span_index = i\n",
    "        res = model(seq, mask, span_batch, span_index)\n",
    "        loss = criterion(res, span_label_batch)    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(loss)\n",
    "        # bs = len(span_label_batch)\n",
    "        '''\n",
    "        if step % 10 == 0:\n",
    "            print(loss)\n",
    "            valid = torch.sum(span_label_batch != 0)\n",
    "            predicted = torch.argmax(res, dim=1)\n",
    "            true_valid = torch.sum((span_label_batch == predicted)*(span_label_batch != 0))\n",
    "        # print(res, span_label_batch)\n",
    "            print(true_valid/valid, true_valid, valid)\n",
    "        '''\n",
    "    print(\"cur results on dev:\", evaluate(model, valid_loader, validset.c_ori_ner))\n",
    "print(\"on test set:\", evaluate(model, test_loader, testset.c_ori_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d14bb870",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T08:54:15.447761Z",
     "start_time": "2021-07-10T08:54:15.434762Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, n_total_ner):\n",
    "    print(\"evaluating...\")\n",
    "    # c_time = time.time()\n",
    "    a_cor, a_tot = 0, n_total_ner\n",
    "    a_pre = 0\n",
    "    l_cor = 0\n",
    "    l_tot = 0\n",
    "    l_pred = 0\n",
    "    # l_total_cand = 0\n",
    "    model.eval()\n",
    "    for l in tqdm(loader):\n",
    "        seq, mask, span_batch, span_label_batch, span_index = i\n",
    "        with torch.no_grad():\n",
    "            res = model(seq, mask, span_batch, span_index)\n",
    "            l_tot += torch.sum(span_label_batch != 0)\n",
    "            a_tot += span_label_batch.shape[0]\n",
    "            predicted = torch.argmax(res, dim=1)\n",
    "            l_pred += torch.sum(predicted != 0)\n",
    "            a_cor += torch.sum(span_label_batch == predicted)\n",
    "            l_cor += torch.sum((span_label_batch == predicted)*(span_label_batch != 0))\n",
    "        \n",
    "    acc = a_cor / a_tot\n",
    "    print('all accuracy: %4f'%acc)\n",
    "    print('for valid spans: cor: %d, pred: %d, tot: %d, cand tot: %d'%(l_cor, l_pred, n_total_ner, l_tot))\n",
    "    p = l_cor / l_pred\n",
    "    r = l_cor / n_total_ner\n",
    "    f1 = 2 * (p * r) / (p + r + 1e-6)\n",
    "    print('P: %.5f, R: %.5f, F1: %.5f'%(p, r, f1))\n",
    "    \n",
    "    model.train()\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db161d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
